 model_config: 
  model_name: muscall
  projection_dim: 512             # dimensionality of the multimodal projection layer
  # if clip, one of [128, 256, 512, 768]
  # if weighted_clip, one of [128, 256, 512, 768]
  temperature: null
  audio: 
    model: ModifiedResNet         # name of the audio backbone model (ModifiedResNet supported)
    pooling: attention            # pooling mechanism to obtain fixed-size audio features. One of [average, attention]
    audio_len_seconds: 20       #30   # max lenght in seconds
    hidden_size: 256              # 
    conv_out_channels: 16         # number of output channels in the resnet conv layers
    n_mels: 128                   # number of mel filterbanks to use in melspectrogram
    sample_rate: 16000            # sample rate of the input audio
    n_fft: 1024                   # size of the FFT
    f_min: 0                      # min frequency in the spectrogram
    f_max: 11025                  # max frequency in the spectrogram
    ssl:
      do_ssl: False               # whether to add audio self-supervised learning during pre-training
      ssl_loss_weight: 0.3
      ssl_temperature: 0.5
      ssl_projection_dim: 128
      p_polarity: 0.8
      p_noise: 0.3
      p_gain: 0.2
      p_pitch_shift: 0.4
  text: 
    model: Bert       
    # name of the textual head model. One of TextTransformer, CLIPTextModel, Bert
    pretrained: bert-base-uncased #openai/clip-vit-base-patch32
    # if cliptextmodel, one of [openai/clip-vit-base-patch32, openai/clip-vit-base-patch32]
    # if bert, one of [bert-base-uncased, bert-large-uncased]
    # if roberta, one of [roberta-base, roberta-large]
    # name of the pretrained textual head model
    frozen_layers: 4         # range of layers to freeze during pretraining
    # one of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 
    num_hidden_layers: 4 #4          
    # if bert, one of [4, 6, 8, 10, 12]
    # if texttransformer, one of [2, 4, 6, 8, 10, 12]
    # if cliptextmodel, one of [2, 4, 6, 8, 10, 12]
    # if roberta, one of [4, 6, 8, 10, 12]
    # number of hidden layers in the transformer
    hidden_size: 768 #512              # dimensionality of the transformer layers
    # if bert, one of [128, 256, 512, 768]
    # if texttransformer, one of [128, 256, 512, 768]
    # if cliptextmodel, one of [128, 256, 512, 768]
    # if roberta, one of [128, 256, 512, 768]

    num_attention_heads: 12 #8        # number of attention heads in the transformer
    # if bert, one of [2, 4, 8, 12]
    # if texttransformer, one of [2, 4, 8, 12]
    # if cliptextmodel, one of [2, 4, 8, 12]
    # if roberta, one of [2, 4, 8, 12]

    vocab_size: 50257 #49408             # size of the vocabulary
    # if bert, one of [30522, 50257]
    # if texttransformer, one of [49408]
    # if cliptextmodel, one of [49408]
    # if roberta, one of [50265, 50257]

    max_position_embeddings: 512 #77   # max number of tokens (seq is truncated if longer)
    # if bert, one of [512, 1024]
    # if texttransformer, one of [77]
    # if cliptextmodel, one of [77]
    # if roberta, one of [512, 1024]

    attention_dropout: 0.2        # dropout probability for the attention weights
    dropout: 0.2                  # dropout probaility in the feedforward blocks
  loss: clip                      # one of [clip, weighted_clip]
  # loss function to use for training
  # clip: CLIP loss
  # weighted_clip: CLIP loss with class weights
  # contrastive: contrastive loss
  
